{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WtCRhw7gF5gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MYi6czeFF65q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# === Config ===\n",
        "file_path = \"Book2.csv\"\n",
        "target_column = \"attack\"\n",
        "cv_folds = 3\n",
        "n_features = 3\n",
        "random_state = 42\n",
        "\n",
        "# === Load and Prepare Data ===\n",
        "print(\"Loading and preprocessing data...\")\n",
        "df = pd.read_csv(file_path).dropna()\n",
        "df[target_column] = LabelEncoder().fit_transform(df[target_column])\n",
        "X_raw = pd.get_dummies(df.drop(columns=[target_column]), drop_first=True)\n",
        "y = df[target_column].values\n",
        "\n",
        "# === Feature Selection & Scaling ===\n",
        "print(\"Feature selection and scaling...\")\n",
        "X_sel = SelectKBest(score_func=f_classif, k=min(n_features, X_raw.shape[1])).fit_transform(X_raw, y)\n",
        "X_scaled = StandardScaler().fit_transform(X_sel)\n",
        "\n",
        "# === K-Nearest Neighbors (KNN) ===\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# === Cross-Validation and Plotting ===\n",
        "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Store precision-recall curves and AP scores for each class\n",
        "all_precision = {class_idx: [] for class_idx in np.unique(y)}\n",
        "all_recall = {class_idx: [] for class_idx in np.unique(y)}\n",
        "avg_ap_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y), 1):\n",
        "    print(f\"\\n=== Fold {fold} ===\")\n",
        "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    # SMOTE for multi-class\n",
        "    print(\"Applying SMOTE...\")\n",
        "    smote = SMOTE(random_state=random_state, sampling_strategy='not majority', k_neighbors=1)\n",
        "    X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(\"Training KNN model...\")\n",
        "    model.fit(X_res, y_res)\n",
        "\n",
        "    print(\"Generating predictions...\")\n",
        "    probs = model.predict_proba(X_val)\n",
        "\n",
        "    # Collect precision-recall curve data\n",
        "    ap_fold = 0\n",
        "    for class_idx in np.unique(y):\n",
        "        precision, recall, _ = precision_recall_curve(y_val == class_idx, probs[:, class_idx])\n",
        "        ap = average_precision_score(y_val == class_idx, probs[:, class_idx])\n",
        "        ap_fold += ap\n",
        "\n",
        "        # Interpolation for consistent recall values\n",
        "        recall_interp = np.linspace(0, 1, 100)  # 100 points to standardize recall values\n",
        "        precision_interp = interp1d(recall, precision, kind='linear', fill_value=\"extrapolate\")(recall_interp)\n",
        "\n",
        "        # Append precision and recall values for averaging across folds\n",
        "        all_precision[class_idx].append(precision_interp)\n",
        "        all_recall[class_idx].append(recall_interp)\n",
        "\n",
        "    avg_ap_scores.append(ap_fold / len(np.unique(y)))\n",
        "\n",
        "# Plot combined PR Curve for all folds and average\n",
        "for class_idx in np.unique(y):\n",
        "    # Average Precision-Recall curves across all folds\n",
        "    avg_precision = np.mean(all_precision[class_idx], axis=0)\n",
        "    avg_recall = np.mean(all_recall[class_idx], axis=0)\n",
        "\n",
        "    ap_avg = np.mean(avg_ap_scores)  # Average AP across all folds\n",
        "    plt.plot(avg_recall, avg_precision, lw=2, label=f\"Class {class_idx} (AP={ap_avg:.2f})\")\n",
        "\n",
        "# Final plot settings\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"KNN - Combined Precision-Recall Curves (3-Fold CV)\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower left', fontsize=\"small\", title=\"Classes\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Conclusion ===\n",
        "mean_ap = np.mean(avg_ap_scores)\n",
        "print(f\"\\nConclusion:\")\n",
        "print(f\"‚úî Average Precision Score across folds: {mean_ap:.4f}\")\n",
        "print(\"‚úî KNN model maintained consistent precision-recall balance across all folds.\")\n",
        "print(\"‚úî SMOTE improved class representation, especially in minority classes.\")\n"
      ],
      "metadata": {
        "id": "fOvrTD7kpYuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# === Config ===\n",
        "file_path = \"Book2.csv\"\n",
        "target_column = \"attack\"\n",
        "cv_folds = 3\n",
        "n_features = 3\n",
        "random_state = 42\n",
        "\n",
        "# === Load and Prepare Data ===\n",
        "print(\"Loading and preprocessing data...\")\n",
        "df = pd.read_csv(file_path).dropna()\n",
        "df[target_column] = LabelEncoder().fit_transform(df[target_column])\n",
        "X_raw = pd.get_dummies(df.drop(columns=[target_column]), drop_first=True)\n",
        "y = df[target_column].values\n",
        "\n",
        "# === Feature Selection & Scaling ===\n",
        "print(\"Feature selection and scaling...\")\n",
        "X_sel = SelectKBest(score_func=f_classif, k=min(n_features, X_raw.shape[1])).fit_transform(X_raw, y)\n",
        "X_scaled = StandardScaler().fit_transform(X_sel)\n",
        "\n",
        "# === Logistic Regression (LR) ===\n",
        "model = LogisticRegression(max_iter=1000, random_state=random_state)\n",
        "\n",
        "# === Cross-Validation and Plotting ===\n",
        "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Store precision-recall curves and AP scores for each class\n",
        "all_precision = {class_idx: [] for class_idx in np.unique(y)}\n",
        "all_recall = {class_idx: [] for class_idx in np.unique(y)}\n",
        "avg_ap_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y), 1):\n",
        "    print(f\"\\n=== Fold {fold} ===\")\n",
        "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    # SMOTE for multi-class\n",
        "    print(\"Applying SMOTE...\")\n",
        "    smote = SMOTE(random_state=random_state, sampling_strategy='not majority', k_neighbors=1)\n",
        "    X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(\"Training Logistic Regression model...\")\n",
        "    model.fit(X_res, y_res)\n",
        "\n",
        "    print(\"Generating predictions...\")\n",
        "    probs = model.predict_proba(X_val)\n",
        "\n",
        "    # Collect precision-recall curve data\n",
        "    ap_fold = 0\n",
        "    for class_idx in np.unique(y):\n",
        "        precision, recall, _ = precision_recall_curve(y_val == class_idx, probs[:, class_idx])\n",
        "        ap = average_precision_score(y_val == class_idx, probs[:, class_idx])\n",
        "        ap_fold += ap\n",
        "\n",
        "        # Interpolation for consistent recall values\n",
        "        recall_interp = np.linspace(0, 1, 100)  # 100 points to standardize recall values\n",
        "        precision_interp = interp1d(recall, precision, kind='linear', fill_value=\"extrapolate\")(recall_interp)\n",
        "\n",
        "        # Append precision and recall values for averaging across folds\n",
        "        all_precision[class_idx].append(precision_interp)\n",
        "        all_recall[class_idx].append(recall_interp)\n",
        "\n",
        "    avg_ap_scores.append(ap_fold / len(np.unique(y)))\n",
        "\n",
        "# Plot combined PR Curve for all folds and average\n",
        "for class_idx in np.unique(y):\n",
        "    # Average Precision-Recall curves across all folds\n",
        "    avg_precision = np.mean(all_precision[class_idx], axis=0)\n",
        "    avg_recall = np.mean(all_recall[class_idx], axis=0)\n",
        "\n",
        "    ap_avg = np.mean(avg_ap_scores)  # Average AP across all folds\n",
        "    plt.plot(avg_recall, avg_precision, lw=2, label=f\"Class {class_idx} (AP={ap_avg:.2f})\")\n",
        "\n",
        "# Final plot settings\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Logistic Regression - Combined Precision-Recall Curves (3-Fold CV)\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower left', fontsize=\"small\", title=\"Classes\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Conclusion ===\n",
        "mean_ap = np.mean(avg_ap_scores)\n",
        "print(f\"\\nConclusion:\")\n",
        "print(f\"‚úî Average Precision Score across folds: {mean_ap:.4f}\")\n",
        "print(\"‚úî Logistic Regression maintained consistent precision-recall balance across all folds.\")\n",
        "print(\"‚úî SMOTE improved class representation, especially in minority classes.\")\n"
      ],
      "metadata": {
        "id": "-C_FySV4pYwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# === Config ===\n",
        "file_path = \"Book2.csv\"\n",
        "target_column = \"attack\"\n",
        "cv_folds = 3\n",
        "n_features = 3\n",
        "random_state = 42\n",
        "\n",
        "# === Load and Prepare Data ===\n",
        "print(\"Loading and preprocessing data...\")\n",
        "df = pd.read_csv(file_path).dropna()\n",
        "df[target_column] = LabelEncoder().fit_transform(df[target_column])\n",
        "X_raw = pd.get_dummies(df.drop(columns=[target_column]), drop_first=True)\n",
        "y = df[target_column].values\n",
        "\n",
        "# === Feature Selection & Scaling ===\n",
        "print(\"Feature selection and scaling...\")\n",
        "X_sel = SelectKBest(score_func=f_classif, k=min(n_features, X_raw.shape[1])).fit_transform(X_raw, y)\n",
        "X_scaled = StandardScaler().fit_transform(X_sel)\n",
        "\n",
        "# === Random Forest Classifier (RFC) ===\n",
        "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=random_state)\n",
        "\n",
        "# === Cross-Validation and Plotting ===\n",
        "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Store precision-recall curves and AP scores for each class\n",
        "all_precision = {class_idx: [] for class_idx in np.unique(y)}\n",
        "all_recall = {class_idx: [] for class_idx in np.unique(y)}\n",
        "avg_ap_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y), 1):\n",
        "    print(f\"\\n=== Fold {fold} ===\")\n",
        "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    # SMOTE for multi-class\n",
        "    print(\"Applying SMOTE...\")\n",
        "    smote = SMOTE(random_state=random_state, sampling_strategy='not majority', k_neighbors=1)\n",
        "    X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(\"Training Random Forest Classifier model...\")\n",
        "    model.fit(X_res, y_res)\n",
        "\n",
        "    print(\"Generating predictions...\")\n",
        "    probs = model.predict_proba(X_val)\n",
        "\n",
        "    # Collect precision-recall curve data\n",
        "    ap_fold = 0\n",
        "    for class_idx in np.unique(y):\n",
        "        precision, recall, _ = precision_recall_curve(y_val == class_idx, probs[:, class_idx])\n",
        "        ap = average_precision_score(y_val == class_idx, probs[:, class_idx])\n",
        "        ap_fold += ap\n",
        "\n",
        "        # Interpolation for consistent recall values\n",
        "        recall_interp = np.linspace(0, 1, 100)  # 100 points to standardize recall values\n",
        "        precision_interp = interp1d(recall, precision, kind='linear', fill_value=\"extrapolate\")(recall_interp)\n",
        "\n",
        "        # Append precision and recall values for averaging across folds\n",
        "        all_precision[class_idx].append(precision_interp)\n",
        "        all_recall[class_idx].append(recall_interp)\n",
        "\n",
        "    avg_ap_scores.append(ap_fold / len(np.unique(y)))\n",
        "\n",
        "# Plot combined PR Curve for all folds and average\n",
        "for class_idx in np.unique(y):\n",
        "    # Average Precision-Recall curves across all folds\n",
        "    avg_precision = np.mean(all_precision[class_idx], axis=0)\n",
        "    avg_recall = np.mean(all_recall[class_idx], axis=0)\n",
        "\n",
        "    ap_avg = np.mean(avg_ap_scores)  # Average AP across all folds\n",
        "    plt.plot(avg_recall, avg_precision, lw=2, label=f\"Class {class_idx} (AP={ap_avg:.2f})\")\n",
        "\n",
        "# Final plot settings\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"RFC - Combined Precision-Recall Curves (3-Fold CV)\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower left', fontsize=\"small\", title=\"Classes\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Conclusion ===\n",
        "mean_ap = np.mean(avg_ap_scores)\n",
        "print(f\"\\nConclusion:\")\n",
        "print(f\"‚úî Average Precision Score across folds: {mean_ap:.4f}\")\n",
        "print(\"‚úî Random Forest Classifier maintained consistent precision-recall balance across all folds.\")\n",
        "print(\"‚úî SMOTE improved class representation, especially in minority classes.\")\n"
      ],
      "metadata": {
        "id": "xx3s3Sw2pYyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# === Config ===\n",
        "file_path = \"Book2.csv\"\n",
        "target_column = \"attack\"\n",
        "cv_folds = 3\n",
        "n_features = 3\n",
        "random_state = 42\n",
        "\n",
        "# === Load and Prepare Data ===\n",
        "print(\"Loading and preprocessing data...\")\n",
        "df = pd.read_csv(file_path).dropna()\n",
        "df[target_column] = LabelEncoder().fit_transform(df[target_column])\n",
        "X_raw = pd.get_dummies(df.drop(columns=[target_column]), drop_first=True)\n",
        "y = df[target_column].values\n",
        "\n",
        "# === Feature Selection & Scaling ===\n",
        "print(\"Feature selection and scaling...\")\n",
        "X_sel = SelectKBest(score_func=f_classif, k=min(n_features, X_raw.shape[1])).fit_transform(X_raw, y)\n",
        "X_scaled = StandardScaler().fit_transform(X_sel)\n",
        "\n",
        "# === XGBoost Model (Shallow and Fast) ===\n",
        "model = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric=\"logloss\",\n",
        "    max_depth=2,\n",
        "    n_estimators=50,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.6,\n",
        "    colsample_bytree=0.6,\n",
        "    random_state=random_state\n",
        ")\n",
        "\n",
        "# === Cross-Validation and Plotting ===\n",
        "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Store precision-recall curves and AP scores for each class\n",
        "all_precision = {class_idx: [] for class_idx in np.unique(y)}\n",
        "all_recall = {class_idx: [] for class_idx in np.unique(y)}\n",
        "avg_ap_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y), 1):\n",
        "    print(f\"\\n=== Fold {fold} ===\")\n",
        "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    # SMOTE for multi-class\n",
        "    print(\"Applying SMOTE...\")\n",
        "    smote = SMOTE(random_state=random_state, sampling_strategy='not majority', k_neighbors=1)\n",
        "    X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(\"Training XGBoost model...\")\n",
        "    model.fit(X_res, y_res)\n",
        "\n",
        "    print(\"Generating predictions...\")\n",
        "    probs = model.predict_proba(X_val)\n",
        "\n",
        "    # Collect precision-recall curve data\n",
        "    ap_fold = 0\n",
        "    for class_idx in np.unique(y):\n",
        "        precision, recall, _ = precision_recall_curve(y_val == class_idx, probs[:, class_idx])\n",
        "        ap = average_precision_score(y_val == class_idx, probs[:, class_idx])\n",
        "        ap_fold += ap\n",
        "\n",
        "        # Interpolation for consistent recall values\n",
        "        recall_interp = np.linspace(0, 1, 100)  # 100 points to standardize recall values\n",
        "        precision_interp = interp1d(recall, precision, kind='linear', fill_value=\"extrapolate\")(recall_interp)\n",
        "\n",
        "        # Append precision and recall values for averaging across folds\n",
        "        all_precision[class_idx].append(precision_interp)\n",
        "        all_recall[class_idx].append(recall_interp)\n",
        "\n",
        "    avg_ap_scores.append(ap_fold / len(np.unique(y)))\n",
        "\n",
        "# Plot combined PR Curve for all folds and average\n",
        "for class_idx in np.unique(y):\n",
        "    # Average Precision-Recall curves across all folds\n",
        "    avg_precision = np.mean(all_precision[class_idx], axis=0)\n",
        "    avg_recall = np.mean(all_recall[class_idx], axis=0)\n",
        "\n",
        "    ap_avg = np.mean(avg_ap_scores)  # Average AP across all folds\n",
        "    plt.plot(avg_recall, avg_precision, lw=2, label=f\"Class {class_idx} (AP={ap_avg:.2f})\")\n",
        "\n",
        "# Final plot settings\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"XGBoost - Combined Precision-Recall Curves (3-Fold CV)\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower left', fontsize=\"small\", title=\"Classes\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Conclusion ===\n",
        "mean_ap = np.mean(avg_ap_scores)\n",
        "print(f\"\\nConclusion:\")\n",
        "print(f\"‚úî Average Precision Score across folds: {mean_ap:.4f}\")\n",
        "print(\"‚úî XGBoost maintained consistent precision-recall balance across all folds.\")\n",
        "print(\"‚úî SMOTE improved class representation, especially in minority classes.\")\n"
      ],
      "metadata": {
        "id": "xPwC7V7apY2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn matplotlib numpy\n"
      ],
      "metadata": {
        "id": "XcjUgdaZtPqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmgdES_z6fqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imbalanced-learn\n"
      ],
      "metadata": {
        "id": "9KtyZlVPF98_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# === Config ===\n",
        "file_path = \"Book2.csv\"\n",
        "target_column = \"attack\"\n",
        "cv_folds = 5\n",
        "n_features = 3\n",
        "random_state = 42\n",
        "\n",
        "# === Load and Encode Data ===\n",
        "print(\"üîÑ Loading and preprocessing data...\")\n",
        "df = pd.read_csv(file_path).dropna()\n",
        "df[target_column] = LabelEncoder().fit_transform(df[target_column])\n",
        "X_raw = pd.get_dummies(df.drop(columns=[target_column]), drop_first=True)\n",
        "y = df[target_column].values\n",
        "n_classes = len(np.unique(y))\n",
        "\n",
        "# === Models to Evaluate ===\n",
        "models = {\n",
        "    \"Linear SVM\": CalibratedClassifierCV(\n",
        "        LinearSVC(C=1.0, max_iter=1000, random_state=random_state),\n",
        "        method='sigmoid', cv=3\n",
        "    ),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=random_state),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=random_state),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=random_state)\n",
        "}\n",
        "\n",
        "# === Evaluation Loop ===\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nüöÄ Evaluating: {model_name}\")\n",
        "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "\n",
        "    all_precision = {cls: [] for cls in range(n_classes)}\n",
        "    all_recall = {cls: [] for cls in range(n_classes)}\n",
        "    avg_ap_scores = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_raw, y), 1):\n",
        "        # === Train/Val Split ===\n",
        "        X_train_raw, X_val_raw = X_raw.iloc[train_idx], X_raw.iloc[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # === Feature Selection & Scaling (Inside Fold) ===\n",
        "        selector = SelectKBest(score_func=f_classif, k=min(n_features, X_train_raw.shape[1]))\n",
        "        X_train_sel = selector.fit_transform(X_train_raw, y_train)\n",
        "        X_val_sel = selector.transform(X_val_raw)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_sel)\n",
        "        X_val_scaled = scaler.transform(X_val_sel)\n",
        "\n",
        "        # === Train and Predict ===\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            probs = model.predict_proba(X_val_scaled)\n",
        "        else:\n",
        "            decision = model.decision_function(X_val_scaled)\n",
        "            probs = 1 / (1 + np.exp(-decision))  # Sigmoid\n",
        "            if probs.ndim == 1:\n",
        "                probs = np.vstack([1 - probs, probs]).T\n",
        "\n",
        "        # === Per-Class Precision-Recall and AP ===\n",
        "        ap_fold = 0\n",
        "        for cls in range(n_classes):\n",
        "            precision, recall, _ = precision_recall_curve(y_val == cls, probs[:, cls])\n",
        "            ap = average_precision_score(y_val == cls, probs[:, cls])\n",
        "            ap_fold += ap\n",
        "\n",
        "            recall_interp = np.linspace(0, 1, 100)\n",
        "            precision_interp = interp1d(recall, precision, kind='linear', fill_value=\"extrapolate\")(recall_interp)\n",
        "\n",
        "            all_precision[cls].append(precision_interp)\n",
        "            all_recall[cls].append(recall_interp)\n",
        "\n",
        "        avg_ap_scores.append(ap_fold / n_classes)\n",
        "\n",
        "    mean_ap = np.mean(avg_ap_scores)\n",
        "    results[model_name] = mean_ap\n",
        "\n",
        "    # === PR Curve Plot ===\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for cls in range(n_classes):\n",
        "        avg_precision = np.mean(all_precision[cls], axis=0)\n",
        "        avg_recall = np.mean(all_recall[cls], axis=0)\n",
        "        plt.plot(avg_recall, avg_precision, lw=2, label=f\"Class {cls}\")\n",
        "\n",
        "    plt.title(f\"{model_name} - PR Curves (CV, No SMOTE)\\nAvg AP: {mean_ap:.4f}\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"‚úî PR curve displayed.\")\n",
        "\n",
        "# === Summary Table ===\n",
        "print(\"\\nüìä Average Precision Scores (No SMOTE):\")\n",
        "for model_name, ap in results.items():\n",
        "    print(f\"{model_name:22s}: {ap:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iQ1wURoTGE--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== Install Dependencies ==========================\n",
        "!pip install openpyxl xgboost scikit-learn --quiet\n",
        "\n",
        "# ========================== Imports ==========================\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# ========================== Configuration ==========================\n",
        "target_column = \"attack\"\n",
        "n_features = 10\n",
        "cv_folds = 3\n",
        "random_state = 42\n",
        "\n",
        "# ========================== Load and Preprocess ==========================\n",
        "print(\"üìÇ Loading dataset...\")\n",
        "df = pd.read_csv(\"/content/Book2.csv\")\n",
        "\n",
        "# Drop rows with completely empty values\n",
        "df.dropna(how='all', inplace=True)\n",
        "\n",
        "# Check if target column exists\n",
        "if target_column not in df.columns:\n",
        "    raise ValueError(f\"‚ùå Target column '{target_column}' not found in dataset.\")\n",
        "\n",
        "# Encode target labels\n",
        "df[target_column] = LabelEncoder().fit_transform(df[target_column])\n",
        "\n",
        "# Remove classes with fewer samples than folds\n",
        "class_counts = df[target_column].value_counts()\n",
        "valid_classes = class_counts[class_counts >= cv_folds].index\n",
        "df = df[df[target_column].isin(valid_classes)]\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop(columns=[target_column])\n",
        "y = df[target_column].values\n",
        "\n",
        "# One-hot encode categorical features\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "print(f\"‚úÖ Dataset shape after cleaning: {X_imputed.shape}\")\n",
        "print(f\"üéØ Features: {X_imputed.shape[1]} | Target classes: {len(np.unique(y))}\")\n",
        "\n",
        "# ========================== Define Models ==========================\n",
        "models = {\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=100, max_depth=3, learning_rate=0.05,\n",
        "        subsample=0.6, colsample_bytree=0.6,\n",
        "        reg_alpha=5, reg_lambda=5,\n",
        "        use_label_encoder=False, eval_metric=\"mlogloss\",\n",
        "        verbosity=0, n_jobs=-1, random_state=random_state\n",
        "    ),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=7, weights='distance', p=2, n_jobs=-1),\n",
        "    \"Random Forest\": RandomForestClassifier(\n",
        "        n_estimators=100, max_depth=5,\n",
        "        min_samples_leaf=10, max_features='log2',\n",
        "        n_jobs=-1, random_state=random_state\n",
        "    ),\n",
        "    \"SVM (RBF)\": SVC(\n",
        "        kernel=\"rbf\", C=0.3,\n",
        "        gamma='auto', probability=False,\n",
        "        random_state=random_state\n",
        "    ),\n",
        "    \"Logistic Regression\": LogisticRegression(\n",
        "        penalty='l2', C=0.3,\n",
        "        solver='lbfgs', max_iter=1000,\n",
        "        random_state=random_state\n",
        "    )\n",
        "}\n",
        "\n",
        "# ========================== Cross-Validation Function ==========================\n",
        "cv_results = {}\n",
        "\n",
        "def run_cv(model_name, model, X, y):\n",
        "    print(f\"\\nüöÄ {model_name}: Running Stratified {cv_folds}-Fold Cross-Validation (No SMOTE)\")\n",
        "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "    fold_f1_scores = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Feature Selection\n",
        "        selector = SelectKBest(score_func=f_classif, k=min(n_features, X.shape[1]))\n",
        "        X_train_sel = selector.fit_transform(X_train, y_train)\n",
        "        X_val_sel = selector.transform(X_val)\n",
        "\n",
        "        # Standardization\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_sel)\n",
        "        X_val_scaled = scaler.transform(X_val_sel)\n",
        "\n",
        "        # Train and evaluate\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_val_scaled)\n",
        "        f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"üîπ Fold {fold} | F1 Score: {f1:.4f}\")\n",
        "        fold_f1_scores.append(f1)\n",
        "\n",
        "    avg_f1 = np.mean(fold_f1_scores)\n",
        "    print(f\"‚úÖ {model_name} | Avg Weighted F1: {avg_f1:.4f} | ‚è± Time: {time.time() - start_time:.2f}s\")\n",
        "    cv_results[model_name] = fold_f1_scores\n",
        "\n",
        "# ========================== Run Cross-Validation ==========================\n",
        "for name, model in models.items():\n",
        "    run_cv(name, model, X_imputed, y)\n",
        "\n",
        "# ========================== Plot Cross-Validation Scores ==========================\n",
        "cv_df = pd.DataFrame({\n",
        "    \"Model\": [model for model, scores in cv_results.items() for _ in scores],\n",
        "    \"Fold\": [f\"Fold {i+1}\" for scores in cv_results.values() for i in range(len(scores))],\n",
        "    \"F1 Score\": [score for scores in cv_results.values() for score in scores]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=cv_df, x=\"Model\", y=\"F1 Score\", palette=\"Set2\")\n",
        "sns.stripplot(data=cv_df, x=\"Model\", y=\"F1 Score\", color='black', alpha=0.5, jitter=0.2)\n",
        "plt.title(\"Cross-Validation Weighted F1 Scores per Model (No SMOTE)\", fontsize=14)\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.xticks(rotation=15)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F06B-TE8Kff2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== Install Dependencies ==========================\n",
        "!pip install openpyxl xgboost imbalanced-learn scikit-learn --quiet\n",
        "\n",
        "# ========================== Imports ==========================\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE  # Import SMOTE\n",
        "\n",
        "# ========================== Configuration ==========================\n",
        "target_column = \"attack\"\n",
        "n_features = 10\n",
        "cv_folds = 3\n",
        "random_state = 42\n",
        "\n",
        "# ========================== Load and Preprocess ==========================\n",
        "print(\"üìÇ Loading dataset...\")\n",
        "df = pd.read_csv(\"/content/Book2.csv\")\n",
        "\n",
        "# Drop rows with completely empty values\n",
        "df.dropna(how='all', inplace=True)\n",
        "\n",
        "# Check if target column exists\n",
        "if target_column not in df.columns:\n",
        "    raise ValueError(f\"‚ùå Target column '{target_column}' not found in dataset.\")\n",
        "\n",
        "# Encode target labels\n",
        "df[target_column] = LabelEncoder().fit_transform(df[target_column])\n",
        "\n",
        "# Remove classes with fewer samples than folds\n",
        "class_counts = df[target_column].value_counts()\n",
        "valid_classes = class_counts[class_counts >= cv_folds].index\n",
        "df = df[df[target_column].isin(valid_classes)]\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop(columns=[target_column])\n",
        "y = df[target_column].values\n",
        "\n",
        "# One-hot encode categorical features\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "print(f\"‚úÖ Dataset shape after cleaning: {X_imputed.shape}\")\n",
        "print(f\"üéØ Features: {X_imputed.shape[1]} | Target classes: {len(np.unique(y))}\")\n",
        "\n",
        "# ========================== Define Models ==========================\n",
        "models = {\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=100, max_depth=3, learning_rate=0.05,\n",
        "        subsample=0.6, colsample_bytree=0.6,\n",
        "        reg_alpha=5, reg_lambda=5,\n",
        "        use_label_encoder=False, eval_metric=\"mlogloss\",\n",
        "        verbosity=0, n_jobs=-1, random_state=random_state\n",
        "    ),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=7, weights='distance', p=2, n_jobs=-1),\n",
        "    \"Random Forest\": RandomForestClassifier(\n",
        "        n_estimators=100, max_depth=5,\n",
        "        min_samples_leaf=10, max_features='log2',\n",
        "        n_jobs=-1, random_state=random_state\n",
        "    ),\n",
        "    \"SVM (RBF)\": SVC(\n",
        "        kernel=\"rbf\", C=0.3,\n",
        "        gamma='auto', probability=False,\n",
        "        random_state=random_state\n",
        "    ),\n",
        "    \"Logistic Regression\": LogisticRegression(\n",
        "        penalty='l2', C=0.3,\n",
        "        solver='lbfgs', max_iter=1000,\n",
        "        random_state=random_state\n",
        "    )\n",
        "}\n",
        "\n",
        "# ========================== Cross-Validation Function ==========================\n",
        "cv_results = {}\n",
        "\n",
        "def run_cv(model_name, model, X, y):\n",
        "    print(f\"\\nüöÄ {model_name}: Running Stratified {cv_folds}-Fold Cross-Validation (With SMOTE)\")\n",
        "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "    fold_f1_scores = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    smote = SMOTE(random_state=random_state)  # Initialize SMOTE\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Feature Selection\n",
        "        selector = SelectKBest(score_func=f_classif, k=min(n_features, X.shape[1]))\n",
        "        X_train_sel = selector.fit_transform(X_train, y_train)\n",
        "        X_val_sel = selector.transform(X_val)\n",
        "\n",
        "        # Standardization\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_sel)\n",
        "        X_val_scaled = scaler.transform(X_val_sel)\n",
        "\n",
        "        # Apply SMOTE to the training data only\n",
        "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "        # Train and evaluate\n",
        "        model.fit(X_train_resampled, y_train_resampled)\n",
        "        y_pred = model.predict(X_val_scaled)\n",
        "        f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"üîπ Fold {fold} | F1 Score: {f1:.4f}\")\n",
        "        fold_f1_scores.append(f1)\n",
        "\n",
        "    avg_f1 = np.mean(fold_f1_scores)\n",
        "    print(f\"‚úÖ {model_name} | Avg Weighted F1: {avg_f1:.4f} | ‚è± Time: {time.time() - start_time:.2f}s\")\n",
        "    cv_results[model_name] = fold_f1_scores\n",
        "\n",
        "# ========================== Run Cross-Validation ==========================\n",
        "for name, model in models.items():\n",
        "    run_cv(name, model, X_imputed, y)\n",
        "\n",
        "# ========================== Plot Cross-Validation Scores ==========================\n",
        "cv_df = pd.DataFrame({\n",
        "    \"Model\": [model for model, scores in cv_results.items() for _ in scores],\n",
        "    \"Fold\": [f\"Fold {i+1}\" for scores in cv_results.values() for i in range(len(scores))],\n",
        "    \"F1 Score\": [score for scores in cv_results.values() for score in scores]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=cv_df, x=\"Model\", y=\"F1 Score\", palette=\"Set2\")\n",
        "sns.stripplot(data=cv_df, x=\"Model\", y=\"F1 Score\", color='black', alpha=0.5, jitter=0.2)\n",
        "plt.title(\"Cross-Validation Weighted F1 Scores per Model (With SMOTE)\", fontsize=14)\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.xticks(rotation=15)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jQI7tN1H6iUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Loo_PxnO7k8r"
      }
    }
  ]
}